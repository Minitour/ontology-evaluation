{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,file_name):\n",
    "    \"\"\"\n",
    "    Input  : path and file_name\n",
    "    Purpose: loading text file\n",
    "    Output : list of paragraphs/documents and\n",
    "             title(initial 100 words considred as title of document)\n",
    "    \"\"\"\n",
    "    documents_list = []\n",
    "    titles=[]\n",
    "    with open( os.path.join(path, file_name) ,\"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "    print(\"Total Number of Documents:\",len(documents_list))\n",
    "    titles.append( text[0:min(len(text),100)] )\n",
    "    return documents_list,titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : docuemnt list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents: 214315\n"
     ]
    }
   ],
   "source": [
    "document_list,titles=load_data(\"\",\"../artifacts/text/corpus_all_in_one/corpus_all_in_one.txt\")\n",
    "clean_text=preprocess_data(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "number_of_topics=20\n",
    "words=30000\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Term Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus.reader import Synset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parents(obj):\n",
    "    \"\"\"\n",
    "    Get direct hypernyms\n",
    "    \"\"\"\n",
    "    return obj.hypernyms() + obj.instance_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hierarchies(obj, level=-1):\n",
    "    \n",
    "    if level == 0:\n",
    "        # stop if reached a certain number of levels.\n",
    "        return [[]]\n",
    "    \n",
    "    parents = get_parents(obj)\n",
    "    if not parents:\n",
    "        return [[]]\n",
    "\n",
    "    hierarchies = []\n",
    "    for parent in parents:\n",
    "        tmp = _get_hierarchies(parent, level-1)\n",
    "        for hierarchy in tmp:\n",
    "            hierarchy.append(parent)\n",
    "        hierarchies = hierarchies + tmp\n",
    "    return hierarchies\n",
    "\n",
    "\n",
    "def get_hierarchies(word, level=-1):\n",
    "    normalized = word.lower().replace(' ', '_')\n",
    "    \n",
    "    # a single word may have multiple synsets\n",
    "    entries = wordnet.synsets(normalized, pos=wordnet.NOUN)\n",
    "    filtered_entries = list(filter(lambda x: normalized == x.lemma_names()[0].lower(), entries))\n",
    "\n",
    "    # only use filtered entries if something is left.\n",
    "    if filtered_entries:\n",
    "        entries = filtered_entries\n",
    "\n",
    "    hierarchies = []\n",
    "    for entry in entries:\n",
    "        hierarchies_of_entry = _get_hierarchies(entry, level)\n",
    "        for hierarchy in hierarchies_of_entry:\n",
    "            hierarchy.append(entry)\n",
    "            hierarchies.append(hierarchy)\n",
    "    return hierarchies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('thing.n.12'), Synset('body_of_water.n.01'), Synset('sea.n.01')],\n",
       " [Synset('flow.n.04'), Synset('turbulent_flow.n.01'), Synset('sea.n.03')]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get hypernyms for sea within two levels\n",
    "hierarchies = get_hierarchies('sea', 2)\n",
    "hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying WordNet Expansion on words in LSA clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words: 33754\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for i, _ in model.show_topics():\n",
    "    topic = model.show_topic(i, topn=15000)\n",
    "    for word, score in topic:\n",
    "        unique_words.add(word)\n",
    "        \n",
    "print(f'number of unique words: {len(unique_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words after wordnet expansion 42603\n"
     ]
    }
   ],
   "source": [
    "unique_words_expanded = set()\n",
    "for word in unique_words:\n",
    "    unique_words_expanded.add(word)\n",
    "    \n",
    "    # get wordnet hypernyms of two levels\n",
    "    hierarchies = get_hierarchies(word, 2)\n",
    "    \n",
    "    if not hierarchies:\n",
    "        # no expansion, append word as it is\n",
    "        continue\n",
    "        \n",
    "    # for every item in the hierarchy except the original\n",
    "    for hierarchy in hierarchies[:-1]:\n",
    "        for item in hierarchy:\n",
    "            for lemma_name in item.lemma_names():\n",
    "                unique_words_expanded.add(lemma_name.lower().replace('_', ' '))\n",
    "            \n",
    "\n",
    "print(f'number of unique words after wordnet expansion {len(unique_words_expanded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare against ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.ontology import sparql, walk\n",
    "from evaluation import ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ontology(path: str, kind='OWL'):\n",
    "    concept_set = set()\n",
    "    g = ontology.sparql.graph_from(path)\n",
    "    classes = ontology.sparql.get_classes(g, kind)\n",
    "    \n",
    "    for identifier, term in classes:\n",
    "        concept_set.add(term.lower())\n",
    "    return concept_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_file = 'sweet.owl'\n",
    "ontology_vocabulary = process_ontology(f'../artifacts/ontologies/{ontology_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4524\n"
     ]
    }
   ],
   "source": [
    "print(len(ontology_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positive=1133, false_positive=41470, true_negative=0, false_negative=3279\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0 # was in the ontology and in the corpus\n",
    "false_positive = 0 # was not in the ontology, but was in the corpus\n",
    "true_negative = 0  # always 0\n",
    "false_negative = 0 # was in the ontology, but was not in the corpus\n",
    "\n",
    "\n",
    "for corpus_word in unique_words_expanded:\n",
    "    if corpus_word in ontology_vocabulary:\n",
    "        # was in the ontology and in the corpus\n",
    "        true_positive += 1\n",
    "    else:\n",
    "        # was not in the ontology, but was in the corpus\n",
    "        false_positive += 1\n",
    "    \n",
    "for ontology_term in ontology_vocabulary:\n",
    "    if ontology_term not in unique_words_expanded:\n",
    "        # was in the ontology, but was not in the corpus\n",
    "        false_negative += 1\n",
    "        \n",
    "print(f'{true_positive=}, {false_positive=}, {true_negative=}, {false_negative=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.02659437128840692, recall: 0.2567996373526745\n"
     ]
    }
   ],
   "source": [
    "precision = true_positive / (true_positive + false_positive) # fitness \n",
    "recall = true_positive / (true_positive + false_negative) # unneeded concepts\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
